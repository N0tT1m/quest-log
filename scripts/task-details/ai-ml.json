[
  {
    "pathName": "AI/ML Deep Learning",
    "pathId": 1,
    "tasks": [
      {
        "id": 1,
        "title": "Watch 3Blue1Brown Essence of Linear Algebra",
        "details": "## Overview\nYouTube playlist (16 videos, ~3hrs total). Focus on chapters 1-7: vectors, linear combinations, matrices as transformations, matrix multiplication, determinants. Key insight: matrices are FUNCTIONS that transform space. When you multiply a matrix by a vector, you're applying a transformation. This visual intuition is essential for understanding attention mechanisms later. URL: youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 2,
        "title": "Practice matrix multiplication by hand",
        "details": "## Overview\nMatrix multiplication is fundamental to neural networks. The shape rule (m\u00d7n)@(n\u00d7p)=(m\u00d7p) must be internalized for understanding attention and layer operations.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 3,
        "title": "Understand broadcasting in NumPy/PyTorch",
        "details": "## Overview\nBroadcasting allows operations on tensors of different shapes by automatically expanding dimensions. Rules: align from right, dimensions match if equal or one is 1.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 4,
        "title": "Learn transpose and reshape operations",
        "details": "## Overview\nTranspose swaps dimensions: (batch, seq, dim) \u2192 (batch, dim, seq). Reshape changes shape without changing data: (2,6) \u2192 (3,4) \u2192 (12,). View vs reshape: view shares memory, reshape may copy. In attention: Q,K,V get reshaped from (batch, seq, d_model) to (batch, heads, seq, d_k). Practice: take a (2,3,4) tensor, transpose dims 1 and 2, then reshape to (2,12).\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 5,
        "title": "Watch 3Blue1Brown Essence of Calculus",
        "details": "## Overview\nYouTube playlist, focus on chapters 1-4 (~1hr). Key concepts: derivative = instantaneous rate of change = slope of tangent line. You don't need to compute derivatives by hand - PyTorch does this. But understand: if loss is high, gradient tells you which direction to adjust weights. URL: youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 6,
        "title": "Understand the chain rule conceptually",
        "details": "## Overview\nBackpropagation applies the chain rule to compute gradients through a computation graph. Each operation's gradient is multiplied along the path from output to parameters.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 7,
        "title": "Learn what a gradient is",
        "details": "## Overview\nGradients indicate the direction and magnitude of steepest increase for a function. In optimization, we move opposite to gradients (descent) to minimize loss.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 8,
        "title": "Understand softmax function",
        "details": "## Overview\nSoftmax converts logits to probabilities that sum to 1. It's used in attention weights and classification outputs. Numerical stability requires subtracting the max before exponentiating.\n\n### Implementation\n```python\ndef softmax(x, dim=-1):\n    x_max = x.max(dim=dim, keepdim=True).values\n    exp_x = torch.exp(x - x_max)  # Numerical stability\n    return exp_x / exp_x.sum(dim=dim, keepdim=True)\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 9,
        "title": "Learn cross-entropy loss intuition",
        "details": "## Overview\nLoss functions measure the difference between predictions and targets. Cross-entropy loss is standard for classification: CE = -log(probability of correct class).\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 10,
        "title": "Understand KL divergence basics",
        "details": "## Overview\nKL(P||Q) = \u03a3 P(x) \u00b7 log(P(x)/Q(x)). Measures how different Q is from P. NOT symmetric: KL(P||Q) \u2260 KL(Q||P). Used in: VAEs, RLHF (KL penalty keeps model close to reference), knowledge distillation. Intuition: \"extra bits needed to encode P using code optimized for Q.\" Cross-entropy = entropy + KL divergence.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 11,
        "title": "Study probability distributions",
        "details": "## Overview\nKey distributions: 1) Categorical - discrete choices (token prediction), 2) Normal/Gaussian - continuous, bell curve (weight initialization, VAE latents), 3) Uniform - equal probability (dropout mask). Understand: mean, variance, sampling. In PyTorch: torch.distributions module. Practice: sample from Normal(0,1), compute mean of 1000 samples.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 12,
        "title": "Implement softmax from scratch",
        "details": "## Overview\nSoftmax converts logits to probabilities that sum to 1. It's used in attention weights and classification outputs. Numerical stability requires subtracting the max before exponentiating.\n\n### Implementation\n```python\ndef softmax(x, dim=-1):\n    x_max = x.max(dim=dim, keepdim=True).values\n    exp_x = torch.exp(x - x_max)  # Numerical stability\n    return exp_x / exp_x.sum(dim=dim, keepdim=True)\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 13,
        "title": "Watch 3Blue1Brown Neural Networks playlist",
        "details": "## Overview\n4 videos (~1hr total). Covers: what neural nets compute, gradient descent visualization, backpropagation intuition. Key insight: neural net = layers of simple functions composed together. Each neuron: weighted sum \u2192 activation function. Training = adjusting weights to minimize loss. URL: youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 14,
        "title": "Clone and study micrograd",
        "details": "## Overview\ngit clone https://github.com/karpathy/micrograd. Only ~100 lines of core code in micrograd/engine.py. Implements: Value class with data + grad + _backward function. Operations build a graph, backward() traverses it. Study: __add__, __mul__, __pow__, tanh, backward(). This is ALL autograd is - tracking operations and computing gradients via chain rule.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 15,
        "title": "Reimplement micrograd from memory",
        "details": "## Overview\nClose the repo. Open blank file. Build Value class from scratch: __init__(data), __add__, __mul__, __repr__. Add _children and _op tracking. Implement backward() with topological sort. Test: a = Value(2); b = Value(3); c = a * b + a; c.backward(). Verify a.grad and b.grad are correct. Debug until it works.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 16,
        "title": "Watch Karpathy makemore Part 1-2",
        "details": "## Overview\nPart 1 (1hr): Bigram model - simplest language model, just counts. Part 2 (1hr): MLP language model - embeddings + hidden layer + output. URL: youtube.com/watch?v=PaCmpygFfXo and watch?v=TCH_1BHY58I. Code along in Jupyter. Key concepts: character embeddings, negative log likelihood loss, mini-batch training.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 17,
        "title": "Watch Karpathy makemore Part 3-4",
        "details": "## Overview\nPart 3 (1hr): BatchNorm - why activations blow up, how BN fixes it. Dead neurons, activation statistics. Part 4 (1hr): Manual backprop - compute gradients by hand through the network. URL: watch?v=P6sfmUTpUmc and watch?v=q8SA3rM6ckI. This is where you REALLY understand what PyTorch does automatically.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 18,
        "title": "Read The Little Book of Deep Learning",
        "details": "## Overview\nFree PDF at fleuret.org/francois/lbdl.html. Only 150 pages with figures. Covers: MLPs, CNNs, attention, training techniques, architectures. Read in 2-3 sessions. Skip proofs, focus on intuition and architecture diagrams. Good reference to return to.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 19,
        "title": "Complete micrograd exercises",
        "details": "## Overview\nExtend your micrograd: 1) Add subtraction and division, 2) Add ReLU activation, 3) Add exp() and log(), 4) Build a 2-layer MLP class, 5) Train it on a simple dataset (e.g., XOR problem or sklearn moons). If something breaks, debug by comparing gradients to PyTorch.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 20,
        "title": "Read \"The Illustrated Transformer\"",
        "details": "## Overview\nTransformers are the foundation of modern NLP, using self-attention to process sequences in parallel. Understanding the architecture is essential for working with LLMs.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 21,
        "title": "Watch \"Let's build GPT from scratch\"",
        "details": "## Overview\nKarpathy's 2-hour video: youtube.com/watch?v=kCc8FmEb1nY. Code along in a notebook. Builds character-level GPT from scratch. Covers: self-attention, masking, multi-head, feedforward, residuals, layernorm. Pause frequently. By the end, you'll have working GPT code you understand completely.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 22,
        "title": "Clone nanoGPT repository",
        "details": "## Overview\ngit clone https://github.com/karpathy/nanoGPT. Core file is model.py (~300 lines). Study in order: 1) GPTConfig dataclass, 2) CausalSelfAttention class, 3) MLP class, 4) Block class, 5) GPT class. Trace shapes through forward(). Note: uses Flash Attention when available.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 23,
        "title": "Train nanoGPT on Shakespeare",
        "details": "## Overview\ncd nanoGPT && python data/shakespeare_char/prepare.py && python train.py config/train_shakespeare_char.py. Takes 5-15 min on GPU. Watch loss decrease. Then: python sample.py --out_dir=out-shakespeare-char. Read generated Shakespeare. Try changing: n_layer, n_head, n_embd in config.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 24,
        "title": "Add print statements for tensor shapes",
        "details": "## Overview\nIn model.py forward(), add prints: print(f\"After embed: {x.shape}\"), print(f\"After block {i}: {x.shape}\"). In CausalSelfAttention, print Q, K, V, attention weights shapes. Run one forward pass. Verify: (B, T, C) flows through. Attention weights should be (B, nh, T, T).\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 25,
        "title": "Implement multi-head attention from scratch",
        "details": "## Overview\nAttention mechanisms allow models to focus on relevant parts of the input when producing output. Self-attention computes relationships between all positions in a sequence, enabling context-aware representations.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 26,
        "title": "Implement full transformer block",
        "details": "## Overview\nTransformers are the foundation of modern NLP, using self-attention to process sequences in parallel. Understanding the architecture is essential for working with LLMs.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 27,
        "title": "Build complete GPT model from scratch",
        "details": "## Overview\nCombine everything: 1) Token embedding: nn.Embedding(vocab_size, d_model), 2) Position embedding: nn.Embedding(max_seq_len, d_model), 3) Stack N transformer blocks, 4) Final LayerNorm, 5) Output projection: nn.Linear(d_model, vocab_size). Add generate() method with temperature and top-k sampling.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 28,
        "title": "Study RoPE positional embeddings",
        "details": "## Overview\nEmbeddings map discrete tokens to dense vectors in continuous space. Position embeddings inject sequence order information since attention is position-invariant.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 29,
        "title": "Study and implement RMSNorm",
        "details": "## Overview\nRMSNorm = Root Mean Square Normalization (Llama). Simpler than LayerNorm: no mean subtraction, just divide by RMS. Formula: x * weight / sqrt(mean(x^2) + eps). Faster than LayerNorm, works just as well. Implementation: ~5 lines. class RMSNorm: def forward(x): return x * self.weight * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 30,
        "title": "Implement SwiGLU activation",
        "details": "## Overview\nSwiGLU = Swish-Gated Linear Unit (Llama, PaLM, Mistral). FFN becomes: SwiGLU(x) = Swish(xW1) * (xW2) then project with W3. Swish(x) = x * sigmoid(x). Three weight matrices instead of two. Better than ReLU empirically. Note: d_ff is typically (8/3)*d_model to match parameter count of standard FFN.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 31,
        "title": "Experiment with learning rates",
        "details": "## Overview\nTrain your nanoGPT with lr=1e-3, 1e-4, 1e-5, 1e-2. Log loss curves for each. Observe: too high = loss explodes or oscillates, too low = trains slowly, just right = smooth decrease. Typical ranges: 1e-4 to 1e-3 for small models, 1e-5 to 1e-4 for large models. Create a plot comparing all runs.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 32,
        "title": "Implement learning rate warmup",
        "details": "## Overview\nWarmup: start lr at 0, linearly increase to max_lr over N steps. Then: cosine decay to min_lr. Code: if step < warmup_steps: lr = max_lr * step / warmup_steps; else: lr = min_lr + 0.5*(max_lr-min_lr)*(1+cos(pi*progress)). Typical: 1-2% of training for warmup. Prevents early instability.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 33,
        "title": "Add gradient clipping",
        "details": "## Overview\nGradients indicate the direction and magnitude of steepest increase for a function. In optimization, we move opposite to gradients (descent) to minimize loss.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 34,
        "title": "Study mixed precision training",
        "details": "## Overview\nFP16 = half precision = 2x memory savings + faster on modern GPUs. Challenge: small gradients underflow to 0. Solution: loss scaling - multiply loss by 1000+, gradients scale up, then unscale after backward. PyTorch: torch.cuda.amp.autocast() + GradScaler(). Train with and without - compare speed and final loss.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 35,
        "title": "Read LoRA paper",
        "details": "## Overview\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable matrices to frozen pretrained weights, reducing memory and compute requirements by 10-100x.\n\n### Implementation\n```python\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank=8, alpha=16):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.scale = alpha / rank\n\n    def forward(self, x):\n        return (x @ self.A @ self.B) * self.scale\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 36,
        "title": "Implement LoRA from scratch",
        "details": "## Overview\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable matrices to frozen pretrained weights, reducing memory and compute requirements by 10-100x.\n\n### Implementation\n```python\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank=8, alpha=16):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.scale = alpha / rank\n\n    def forward(self, x):\n        return (x @ self.A @ self.B) * self.scale\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 37,
        "title": "Fine-tune GPT-2 with your LoRA",
        "details": "## Overview\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable matrices to frozen pretrained weights, reducing memory and compute requirements by 10-100x.\n\n### Implementation\n```python\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank=8, alpha=16):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.scale = alpha / rank\n\n    def forward(self, x):\n        return (x @ self.A @ self.B) * self.scale\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 38,
        "title": "Study QLoRA paper",
        "details": "## Overview\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable matrices to frozen pretrained weights, reducing memory and compute requirements by 10-100x.\n\n### Implementation\n```python\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank=8, alpha=16):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.scale = alpha / rank\n\n    def forward(self, x):\n        return (x @ self.A @ self.B) * self.scale\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 39,
        "title": "Read DPO paper",
        "details": "## Overview\nDirect Preference Optimization simplifies RLHF by directly optimizing on preference pairs without a separate reward model, using a clever reparameterization of the RL objective.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 40,
        "title": "Understand RLHF pipeline",
        "details": "## Overview\nRLHF aligns models with human preferences through: 1) supervised fine-tuning on demonstrations, 2) reward model training on preferences, 3) RL optimization with KL penalty.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 41,
        "title": "Implement KV cache",
        "details": "## Overview\nKV caching stores computed Key and Value tensors from previous tokens during autoregressive generation, avoiding redundant computation and dramatically speeding up inference.\n\n### Implementation\n```python\ndef generate_with_cache(model, input_ids, max_new_tokens):\n    past_kv = None\n    for _ in range(max_new_tokens):\n        out = model(input_ids[:, -1:] if past_kv else input_ids, past_kv=past_kv)\n        past_kv = out.past_kv\n        next_token = out.logits[:, -1].argmax(dim=-1, keepdim=True)\n        input_ids = torch.cat([input_ids, next_token], dim=1)\n    return input_ids\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 42,
        "title": "Study Flash Attention",
        "details": "## Overview\nAttention mechanisms allow models to focus on relevant parts of the input when producing output. Self-attention computes relationships between all positions in a sequence, enabling context-aware representations.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 43,
        "title": "Implement top-k and top-p sampling",
        "details": "## Overview\nSampling strategies control generation diversity. Greedy picks highest probability, top-k samples from k best, top-p (nucleus) samples from smallest set with cumulative probability \u2265p.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 44,
        "title": "Study speculative decoding",
        "details": "## Overview\nSpeculative decoding uses a fast draft model to generate candidate tokens, then verifies them in parallel with the large model, achieving 2-3x speedup for autoregressive generation.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 45,
        "title": "Learn quantization basics",
        "details": "## Overview\nQuantization reduces model precision (FP32\u2192INT8\u2192INT4) to decrease memory usage and increase inference speed while maintaining acceptable accuracy.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 46,
        "title": "Study GPTQ/AWQ quantization",
        "details": "## Overview\nQuantization reduces model precision (FP32\u2192INT8\u2192INT4) to decrease memory usage and increase inference speed while maintaining acceptable accuracy.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 47,
        "title": "Try vLLM for serving",
        "details": "## Overview\nHigh-throughput LLM serving requires PagedAttention for efficient KV cache management, continuous batching to maximize GPU utilization, and optimized CUDA kernels.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 48,
        "title": "Implement continuous batching",
        "details": "## Overview\nTraditional: wait for all requests in batch to finish. Continuous: as requests finish, immediately add new ones. Don't let GPU idle waiting for long sequences. Track per-request state: input_ids, kv_cache, generated_tokens. Each step: batch forward pass, some requests may finish (hit EOS or max_len), add new requests. Result: much higher throughput.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 49,
        "title": "Build character-level text generator",
        "details": "## Overview\nTake your GPT implementation and train on custom data: your own writing, a favorite book (Project Gutenberg), code from your repos, or song lyrics. Prepare data: concatenate all text, create train/val split. Train for 5k-20k iterations. Generate samples at different temperatures. Goal: model captures the \"style\" of your corpus.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 50,
        "title": "Create domain-specific fine-tuned model",
        "details": "## Overview\nFine-tuning adapts pretrained models to specific tasks. Full fine-tuning updates all parameters; parameter-efficient methods like LoRA update only a small subset.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 51,
        "title": "Build semantic search engine",
        "details": "## Overview\nUse sentence-transformers: from sentence_transformers import SentenceTransformer; model = SentenceTransformer('all-MiniLM-L6-v2'). Embed your documents (code files, notes, bookmarks). Store embeddings in numpy array or FAISS. Query: embed question, find top-k similar documents via cosine similarity. Build simple UI with Gradio or Streamlit.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 52,
        "title": "Implement recommendation system",
        "details": "## Overview\nCollaborative filtering: matrix of users \u00d7 items, predict missing ratings using matrix factorization (SVD) or neural embeddings. Content-based: embed item descriptions, recommend similar items to what user liked. Hybrid: combine both signals. Dataset: MovieLens (movies), your own watch history, or GitHub stars. Evaluate with held-out data.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 53,
        "title": "Build local LLM inference server",
        "details": "## Overview\nServe a quantized model locally with REST API. Stack: FastAPI + llama.cpp (or vLLM). Quantize a 7B model to 4-bit (~4GB). Endpoints: /generate (streaming), /embeddings. Add: request queue, concurrent handling, basic auth. Test: curl requests, measure latency and throughput. Goal: your own local ChatGPT-like API.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "ML Engineering & Ops",
    "pathId": 3,
    "tasks": [
      {
        "id": 94,
        "title": "Set up feature store template",
        "details": "## Overview\nFeature store = centralized repository for ML features. Build class with: write_features(entity_id, features_dict, timestamp), get_features(entity_id, feature_names, point_in_time), list_features(). Storage: SQLite for simple, Redis for real-time, Feast for production. Key concept: point-in-time correctness - don't leak future data. Template should handle feature versioning.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 95,
        "title": "Implement ETL pipeline base class",
        "details": "## Overview\nAbstract base class: class Pipeline: extract() \u2192 raw data, transform(raw) \u2192 processed data, load(processed) \u2192 destination. Implement for each data source. Add: logging, error handling, retry logic, idempotency (safe to re-run). Use: Airflow for scheduling, or simple cron. Test: pipeline should be restartable at any point without duplicating data.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 96,
        "title": "Create data validation pipeline",
        "details": "## Overview\nValidate before training: 1) Schema validation - column names, types, nullable, 2) Statistical checks - value ranges, distributions, null rates, 3) Data drift detection - compare to baseline statistics. Libraries: Great Expectations, Pandera, or custom. Fail pipeline if validation fails. Log statistics for monitoring. Catch bad data before it poisons your model.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 97,
        "title": "Build streaming data processor",
        "details": "## Overview\nReal-time feature updates. Options: Kafka + Flink (heavy), Redis Streams (medium), Python asyncio (light). Pattern: consume event \u2192 compute features \u2192 update feature store \u2192 trigger model if needed. Handle: late arrivals, out-of-order events, exactly-once processing. Start simple: Redis pub/sub with async Python consumer.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 98,
        "title": "Implement data versioning",
        "details": "## Overview\nDVC (Data Version Control): dvc init, dvc add data.csv, git commit, dvc push to S3/GCS. Now data is versioned like code. Alternative: Delta Lake for parquet (time travel queries), or just timestamp your datasets. Key: reproduce any experiment by checking out code + data version. Test: can you restore dataset from 3 months ago?\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 99,
        "title": "Set up MLflow tracking",
        "details": "## Overview\npip install mlflow, mlflow server --backend-store-uri sqlite:///mlflow.db. In code: mlflow.set_experiment(\"my-experiment\"); with mlflow.start_run(): mlflow.log_params({\"lr\": 0.01}); mlflow.log_metrics({\"loss\": 0.5}); mlflow.sklearn.log_model(model, \"model\"). Model registry: promote models through stages (staging \u2192 production). UI at localhost:5000 shows all experiments.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 100,
        "title": "Implement hyperparameter tuning",
        "details": "## Overview\nOptuna: import optuna; def objective(trial): lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True); model = train(lr); return val_loss. study = optuna.create_study(); study.optimize(objective, n_trials=100). Features: pruning (stop bad trials early), visualization, parallel trials. Log best params to MLflow. Compare to grid search: Optuna is 3-10x more efficient.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 101,
        "title": "Build cross-validation pipeline",
        "details": "## Overview\nFor tabular: sklearn KFold or StratifiedKFold. For time series: TimeSeriesSplit (don't shuffle!). Pipeline: split data \u2192 train on fold \u2192 evaluate \u2192 aggregate metrics. Log each fold to MLflow. Report mean \u00b1 std across folds. Important: tune hyperparams in inner loop, evaluate on outer loop (nested CV) to avoid overfitting to validation set.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 102,
        "title": "Create model comparison framework",
        "details": "## Overview\nCompare models fairly: same data splits, same metrics, same compute budget. Build table: Model | Train Time | Val Loss | Test Loss | Inference Time | Model Size. Statistical significance: paired t-test or bootstrap on fold results. Dashboard: Streamlit or Grafana showing current models and metrics. Automate: new model trains \u2192 auto-compare to baseline.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 103,
        "title": "Implement distributed training",
        "details": "## Overview\nPyTorch DDP: torchrun --nproc_per_node=2 train.py. In code: dist.init_process_group(\"nccl\"); model = DDP(model); sampler = DistributedSampler(dataset). Gradient sync is automatic. Multi-node: set MASTER_ADDR, MASTER_PORT. Mixed precision: torch.cuda.amp.autocast(). Monitor: GPU utilization should be >80%. Common issues: unbalanced batches, gradient accumulation.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 104,
        "title": "Build FastAPI inference server",
        "details": "## Overview\nfrom fastapi import FastAPI; app = FastAPI(); @app.post(\"/predict\") async def predict(data: PredictRequest): return model.predict(data.features). Add: request validation with Pydantic, async for I/O bound ops, background tasks for logging. Deployment: uvicorn + gunicorn, or Docker. Load model once at startup. Health check endpoint. OpenAPI docs auto-generated.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 105,
        "title": "Implement model versioning",
        "details": "## Overview\nBlue-green: run v1 and v2 simultaneously, route traffic gradually. Implementation: /predict?version=2 or header-based routing. Model storage: MLflow registry with stages, or S3 with version prefixes. Rollback: instant switch back to v1 if v2 fails. A/B testing: 10% to v2, compare metrics, ramp up if good. Track: which version served each request.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 106,
        "title": "Add monitoring and alerting",
        "details": "## Overview\nEvidently: from evidently.metrics import DataDriftPreset; report = Report([DataDriftPreset()]); report.run(reference=train_data, current=today_data). Monitor: 1) Input drift - features changing distribution, 2) Prediction drift - outputs changing, 3) Concept drift - relationship between X and Y changing. Alert: Slack/PagerDuty when drift detected. Dashboard: Grafana + Prometheus.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 107,
        "title": "Create load testing suite",
        "details": "## Overview\nLocust: from locust import HttpUser, task; class Predictor(HttpUser): @task def predict(self): self.client.post(\"/predict\", json={...}). Run: locust -f loadtest.py. Measure: requests/sec, p50/p95/p99 latency, error rate. Test scenarios: normal load, burst, sustained high load. Find breaking point. Optimize: batching, caching, model quantization.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 108,
        "title": "Implement graceful degradation",
        "details": "## Overview\nWhen model fails: 1) Return cached prediction if available, 2) Fall back to simpler model (e.g., rules-based), 3) Return safe default with confidence=0, 4) Queue request for async processing. Circuit breaker pattern: if error rate > threshold, stop calling model, return fallback. Timeouts: don't let slow predictions block. Test: kill model process, verify graceful handling.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "Deep Learning Advanced Topics",
    "pathId": 8,
    "tasks": [
      {
        "id": 1492,
        "title": "Understand neural network basics",
        "details": "## Overview\nLayers, activations, backprop\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1493,
        "title": "Implement gradient descent",
        "details": "## Overview\nGradients indicate the direction and magnitude of steepest increase for a function. In optimization, we move opposite to gradients (descent) to minimize loss.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1494,
        "title": "Build simple neural network",
        "details": "## Overview\nForward and backward pass\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1495,
        "title": "Learn PyTorch/TensorFlow basics",
        "details": "## Overview\nFramework fundamentals\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1496,
        "title": "Understand loss functions",
        "details": "## Overview\nLoss functions measure the difference between predictions and targets. Cross-entropy loss is standard for classification: CE = -log(probability of correct class).\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1497,
        "title": "Implement attention mechanism",
        "details": "## Overview\nAttention mechanisms allow models to focus on relevant parts of the input when producing output. Self-attention computes relationships between all positions in a sequence, enabling context-aware representations.\n\n### Implementation\n```python\nimport torch\nimport torch.nn.functional as F\nimport math\n\ndef attention(Q, K, V, mask=None):\n    d_k = Q.size(-1)\n    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n    weights = F.softmax(scores, dim=-1)\n    return torch.matmul(weights, V)\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1498,
        "title": "Build transformer block",
        "details": "## Overview\nTransformers are the foundation of modern NLP, using self-attention to process sequences in parallel. Understanding the architecture is essential for working with LLMs.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1499,
        "title": "Add positional encoding",
        "details": "## Overview\nPosition information\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1500,
        "title": "Implement layer normalization",
        "details": "## Overview\nStabilize training\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1501,
        "title": "Build full model",
        "details": "## Overview\nStack components together\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1502,
        "title": "Set up training loop",
        "details": "## Overview\nBatch processing, logging\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1503,
        "title": "Implement learning rate scheduling",
        "details": "## Overview\nWarmup, decay\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1504,
        "title": "Add model checkpointing",
        "details": "## Overview\nSave and load models\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1505,
        "title": "Build inference pipeline",
        "details": "## Overview\nEfficient prediction\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1506,
        "title": "Deploy model",
        "details": "## Overview\nServe predictions\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "Advanced Deep Learning: Entertainment AI & Production",
    "pathId": 17,
    "tasks": [
      {
        "id": 1156,
        "title": "Implement multi-head attention",
        "details": "## Overview\nAttention mechanisms allow models to focus on relevant parts of the input when producing output. Self-attention computes relationships between all positions in a sequence, enabling context-aware representations.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1157,
        "title": "Build positional encoding",
        "details": "## Overview\nSinusoidal and learned\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1158,
        "title": "Add layer normalization",
        "details": "## Overview\nPre-LN vs Post-LN\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1159,
        "title": "Implement feed-forward network",
        "details": "## Overview\nMLP layers\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1160,
        "title": "Build residual connections",
        "details": "## Overview\nSkip connections\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1161,
        "title": "Implement gradient accumulation",
        "details": "## Overview\nGradients indicate the direction and magnitude of steepest increase for a function. In optimization, we move opposite to gradients (descent) to minimize loss.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1162,
        "title": "Add learning rate warmup",
        "details": "## Overview\nGradual LR increase\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1163,
        "title": "Build mixed precision training",
        "details": "## Overview\nFP16/BF16\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1164,
        "title": "Implement gradient clipping",
        "details": "## Overview\nGradients indicate the direction and magnitude of steepest increase for a function. In optimization, we move opposite to gradients (descent) to minimize loss.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1165,
        "title": "Add checkpoint saving",
        "details": "## Overview\nResume training\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1166,
        "title": "Implement LoRA",
        "details": "## Overview\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable matrices to frozen pretrained weights, reducing memory and compute requirements by 10-100x.\n\n### Implementation\n```python\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank=8, alpha=16):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.scale = alpha / rank\n\n    def forward(self, x):\n        return (x @ self.A @ self.B) * self.scale\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1167,
        "title": "Build QLoRA",
        "details": "## Overview\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable matrices to frozen pretrained weights, reducing memory and compute requirements by 10-100x.\n\n### Implementation\n```python\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank=8, alpha=16):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.scale = alpha / rank\n\n    def forward(self, x):\n        return (x @ self.A @ self.B) * self.scale\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1168,
        "title": "Add prompt tuning",
        "details": "## Overview\nSoft prompts\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1169,
        "title": "Implement adapter layers",
        "details": "## Overview\nBottleneck adapters\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1170,
        "title": "Build instruction tuning pipeline",
        "details": "## Overview\nFollowing instructions\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "Complete ML Pipeline: Data to Deployment",
    "pathId": 26,
    "tasks": [
      {
        "id": 1191,
        "title": "Build data ingestion",
        "details": "## Overview\nMultiple sources\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1192,
        "title": "Implement data validation",
        "details": "## Overview\nGreat Expectations\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1193,
        "title": "Add feature engineering",
        "details": "## Overview\nTransform features\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1194,
        "title": "Build feature store",
        "details": "## Overview\nFeast or similar\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1195,
        "title": "Implement data versioning",
        "details": "## Overview\nDVC\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1196,
        "title": "Set up experiment tracking",
        "details": "## Overview\nMLflow\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1197,
        "title": "Build model registry",
        "details": "## Overview\nVersion models\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1198,
        "title": "Implement CI/CD for ML",
        "details": "## Overview\nAutomated pipelines\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1199,
        "title": "Add model monitoring",
        "details": "## Overview\nDrift detection\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1200,
        "title": "Build A/B testing framework",
        "details": "## Overview\nCompare models\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "Markdown to HTML Compiler",
    "pathId": 39,
    "tasks": [
      {
        "id": 815,
        "title": "Parse inline elements",
        "details": "## Overview\nLinks, emphasis, code\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 817,
        "title": "Handle code blocks",
        "details": "## Overview\nFenced and indented\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 818,
        "title": "Parse tables",
        "details": "## Overview\nGFM table syntax\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 822,
        "title": "Generate table of contents",
        "details": "## Overview\nAuto-generate TOC\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 823,
        "title": "Add front matter parsing",
        "details": "## Overview\nYAML metadata\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 824,
        "title": "Implement template system",
        "details": "## Overview\nWrap in HTML template\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 825,
        "title": "Build CLI tool",
        "details": "## Overview\nCommand-line interface\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "AI Model Opportunities",
    "pathId": 42,
    "tasks": [
      {
        "id": 329,
        "title": "Study 10 successful AI products and document their value proposition",
        "details": "## Overview\nAnalyze products like GitHub Copilot, Midjourney, Jasper - what problem do they solve?\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 330,
        "title": "Map the AI landscape: foundation models, fine-tuned models, agents",
        "details": "## Overview\nFine-tuning adapts pretrained models to specific tasks. Full fine-tuning updates all parameters; parameter-efficient methods like LoRA update only a small subset.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 331,
        "title": "Identify 5 workflows in your domain that could be AI-augmented",
        "details": "## Overview\nDocument current pain points, time spent, and potential AI solutions\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 332,
        "title": "Research API costs and calculate unit economics for each idea",
        "details": "## Overview\nPrice out OpenAI/Anthropic/local model costs per 1000 users\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 333,
        "title": "Build a CLI tool that uses an LLM API for your top idea",
        "details": "## Overview\nFocus on the core value prop - no UI needed yet\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 334,
        "title": "Add prompt engineering: system prompts, few-shot examples, output parsing",
        "details": "## Overview\nIterate on prompts to improve reliability and output quality\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 335,
        "title": "Implement structured output with JSON mode or function calling",
        "details": "## Overview\nMake the AI output parseable and actionable data\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 336,
        "title": "Add error handling, retries, and fallbacks",
        "details": "## Overview\nHandle rate limits, API failures, and malformed responses gracefully\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 337,
        "title": "Build a simple web UI (Streamlit/Gradio) for your prototype",
        "details": "## Overview\nCreate shareable interface for user testing\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 338,
        "title": "Get 5 people to use it and document their feedback",
        "details": "## Overview\nWatch them use it, note confusion points and feature requests\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 339,
        "title": "Measure key metrics: task completion rate, time saved, satisfaction",
        "details": "## Overview\nQuantify the value your tool provides\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 340,
        "title": "Iterate based on feedback - 3 improvement cycles minimum",
        "details": "## Overview\nShip improvements, re-test, repeat\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 341,
        "title": "Implement caching to reduce API costs",
        "details": "## Overview\nCache common queries, use semantic similarity for cache hits\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 342,
        "title": "Add usage tracking and analytics",
        "details": "## Overview\nTrack which features are used, monitor costs per user\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 343,
        "title": "Evaluate local/fine-tuned models vs API tradeoffs",
        "details": "## Overview\nFine-tuning adapts pretrained models to specific tasks. Full fine-tuning updates all parameters; parameter-efficient methods like LoRA update only a small subset.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 344,
        "title": "Document your learnings: what worked, what failed, why",
        "details": "## Overview\nCreate a playbook for your next AI product\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "Deep Learning Guide Part 2",
    "pathId": 43,
    "tasks": [
      {
        "id": 345,
        "title": "Implement ResNet with skip connections from scratch",
        "details": "## Overview\nUnderstand why skip connections enable deeper networks\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 346,
        "title": "Build a U-Net for image segmentation",
        "details": "## Overview\nLearn encoder-decoder architectures with skip connections\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 347,
        "title": "Implement attention mechanisms in CNNs (CBAM, SE-Net)",
        "details": "## Overview\nAttention mechanisms allow models to focus on relevant parts of the input when producing output. Self-attention computes relationships between all positions in a sequence, enabling context-aware representations.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 348,
        "title": "Build a Vision Transformer (ViT) from the paper",
        "details": "## Overview\nTransformers are the foundation of modern NLP, using self-attention to process sequences in parallel. Understanding the architecture is essential for working with LLMs.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 349,
        "title": "Implement and compare optimizers: SGD, Adam, AdamW, LAMB",
        "details": "## Overview\nUnderstand momentum, adaptive learning rates, weight decay\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 350,
        "title": "Implement learning rate schedules: warmup, cosine annealing, OneCycleLR",
        "details": "## Overview\nExperiment with different schedules on the same model\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 351,
        "title": "Study gradient flow: implement gradient clipping, gradient checkpointing",
        "details": "## Overview\nGradients indicate the direction and magnitude of steepest increase for a function. In optimization, we move opposite to gradients (descent) to minimize loss.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 352,
        "title": "Implement mixed precision training (FP16/BF16)",
        "details": "## Overview\nUse torch.cuda.amp for faster training with lower memory\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 353,
        "title": "Implement data augmentation pipelines (albumentations, torchvision)",
        "details": "## Overview\nRandom crops, flips, color jitter, mixup, cutout\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 354,
        "title": "Compare dropout, droppath, and stochastic depth",
        "details": "## Overview\nImplement each and measure impact on generalization\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 355,
        "title": "Implement label smoothing and knowledge distillation",
        "details": "## Overview\nSoft targets for better generalization\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 356,
        "title": "Study the lottery ticket hypothesis - implement magnitude pruning",
        "details": "## Overview\nFind sparse subnetworks that train as well as dense networks\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 357,
        "title": "Implement DataParallel and understand its limitations",
        "details": "## Overview\nSimple multi-GPU but with GIL and memory imbalance issues\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 358,
        "title": "Convert to DistributedDataParallel (DDP)",
        "details": "## Overview\nProper multi-GPU training with gradient synchronization\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 359,
        "title": "Implement gradient accumulation for large effective batch sizes",
        "details": "## Overview\nGradients indicate the direction and magnitude of steepest increase for a function. In optimization, we move opposite to gradients (descent) to minimize loss.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 360,
        "title": "Profile training: identify bottlenecks in data loading, compute, communication",
        "details": "## Overview\nUse PyTorch profiler and tensorboard to optimize throughput\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 361,
        "title": "Implement GradCAM for CNN visualization",
        "details": "## Overview\nSee which image regions drive predictions\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 362,
        "title": "Analyze learned representations with t-SNE and UMAP",
        "details": "## Overview\nVisualize embedding spaces and cluster structure\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 363,
        "title": "Implement influence functions to find training examples affecting predictions",
        "details": "## Overview\nDebug model behavior by tracing to training data\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 364,
        "title": "Build a model debugging dashboard with Weights & Biases",
        "details": "## Overview\nDebuggers use ptrace to control process execution, set breakpoints via INT3 instructions, and inspect memory/registers. DWARF format provides debug symbol information.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "Learn Deep Learning Without Courses",
    "pathId": 45,
    "tasks": [
      {
        "id": 388,
        "title": "Review linear algebra: vectors, matrices, eigenvalues (3Blue1Brown)",
        "details": "## Overview\nWatch Essence of Linear Algebra series and do practice problems\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 389,
        "title": "Review calculus: derivatives, chain rule, gradients",
        "details": "## Overview\nGradients indicate the direction and magnitude of steepest increase for a function. In optimization, we move opposite to gradients (descent) to minimize loss.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 390,
        "title": "Study probability basics: distributions, Bayes theorem, expectation",
        "details": "## Overview\nFoundation for understanding loss functions and generative models\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 391,
        "title": "Implement backpropagation from scratch in NumPy",
        "details": "## Overview\nBackpropagation applies the chain rule to compute gradients through a computation graph. Each operation's gradient is multiplied along the path from output to parameters.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 392,
        "title": "Read and implement the perceptron algorithm",
        "details": "## Overview\nUnderstand the simplest neural unit\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 393,
        "title": "Implement MLPs for MNIST classification",
        "details": "## Overview\nMulti-layer networks, activation functions, softmax\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 394,
        "title": "Study and implement different loss functions",
        "details": "## Overview\nLoss functions measure the difference between predictions and targets. Cross-entropy loss is standard for classification: CE = -log(probability of correct class).\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 395,
        "title": "Implement batch normalization from the paper",
        "details": "## Overview\nRead Ioffe & Szegedy 2015, implement, verify on a task\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 396,
        "title": "Study the vanishing gradient problem and solutions",
        "details": "## Overview\nGradients indicate the direction and magnitude of steepest increase for a function. In optimization, we move opposite to gradients (descent) to minimize loss.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 397,
        "title": "Read and implement LeNet-5 (LeCun 1998)",
        "details": "## Overview\nThe original CNN architecture for digit recognition\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 398,
        "title": "Read AlexNet paper and understand its innovations",
        "details": "## Overview\nReLU, dropout, data augmentation, GPU training\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 399,
        "title": "Implement VGG and understand depth vs width tradeoffs",
        "details": "## Overview\nSimple architecture, powerful features\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 400,
        "title": "Read and implement ResNet (He et al. 2015)",
        "details": "## Overview\nSkip connections enabling very deep networks\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 401,
        "title": "Train a CNN on CIFAR-10 and achieve >90% accuracy",
        "details": "## Overview\nApply everything learned to a real benchmark\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 402,
        "title": "Implement vanilla RNN and understand its limitations",
        "details": "## Overview\nVanishing gradients in long sequences\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 403,
        "title": "Read and implement LSTM (Hochreiter & Schmidhuber 1997)",
        "details": "## Overview\nGates, cell state, and long-term memory\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 404,
        "title": "Implement character-level language model with LSTM",
        "details": "## Overview\nGenerate text character by character\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 405,
        "title": "Read \"Attention Is All You Need\" paper",
        "details": "## Overview\nAttention mechanisms allow models to focus on relevant parts of the input when producing output. Self-attention computes relationships between all positions in a sequence, enabling context-aware representations.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 406,
        "title": "Implement multi-head attention from scratch",
        "details": "## Overview\nAttention mechanisms allow models to focus on relevant parts of the input when producing output. Self-attention computes relationships between all positions in a sequence, enabling context-aware representations.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 407,
        "title": "Study transfer learning and fine-tuning strategies",
        "details": "## Overview\nFine-tuning adapts pretrained models to specific tasks. Full fine-tuning updates all parameters; parameter-efficient methods like LoRA update only a small subset.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 408,
        "title": "Implement a training pipeline with proper validation",
        "details": "## Overview\nTrain/val/test splits, early stopping, model selection\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 409,
        "title": "Read and summarize 5 papers from your area of interest",
        "details": "## Overview\nPractice extracting key ideas and contributions\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 410,
        "title": "Reproduce results from a recent paper",
        "details": "## Overview\nFull implementation and verification of claimed results\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 411,
        "title": "Write up your learning journey as a blog post or notes",
        "details": "## Overview\nTeaching others solidifies your understanding\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "ML Pipeline Complete Guide",
    "pathId": 46,
    "tasks": [
      {
        "id": 412,
        "title": "Set up data versioning with DVC",
        "details": "## Overview\nTrack data changes alongside code changes\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 413,
        "title": "Build data validation checks (great_expectations or pandera)",
        "details": "## Overview\nCatch data quality issues before they break models\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 414,
        "title": "Implement data preprocessing pipeline with clear stages",
        "details": "## Overview\nCleaning, transformation, feature extraction as separate steps\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 415,
        "title": "Handle data imbalance: undersampling, oversampling, SMOTE",
        "details": "## Overview\nSampling strategies control generation diversity. Greedy picks highest probability, top-k samples from k best, top-p (nucleus) samples from smallest set with cumulative probability \u2265p.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 416,
        "title": "Create train/val/test splits with proper stratification",
        "details": "## Overview\nAvoid data leakage and ensure representative splits\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 417,
        "title": "Implement numerical feature transformations",
        "details": "## Overview\nScaling, normalization, log transforms, binning\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 418,
        "title": "Implement categorical encoding strategies",
        "details": "## Overview\nOne-hot, target encoding, embeddings for high cardinality\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 419,
        "title": "Create time-based features for temporal data",
        "details": "## Overview\nLags, rolling windows, seasonal decomposition\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 420,
        "title": "Implement feature selection methods",
        "details": "## Overview\nCorrelation analysis, mutual information, recursive feature elimination\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 421,
        "title": "Build a feature store pattern for reusable features",
        "details": "## Overview\nCentralize feature computation for training and serving\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 422,
        "title": "Set up experiment tracking (MLflow or Weights & Biases)",
        "details": "## Overview\nTrack hyperparameters, metrics, and artifacts\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 423,
        "title": "Implement cross-validation with proper handling of time series",
        "details": "## Overview\nK-fold, stratified, time-series split\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 424,
        "title": "Compare multiple model types on your problem",
        "details": "## Overview\nLinear, tree-based, neural network baselines\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 425,
        "title": "Implement hyperparameter tuning (Optuna or Ray Tune)",
        "details": "## Overview\nBayesian optimization, early stopping, parallel trials\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 426,
        "title": "Analyze model errors and iterate on features/model",
        "details": "## Overview\nError analysis, confusion matrices, calibration curves\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 427,
        "title": "Package model for deployment (ONNX or TorchScript)",
        "details": "## Overview\nExport model to portable format\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 428,
        "title": "Build REST API for model serving (FastAPI)",
        "details": "## Overview\nHigh-throughput LLM serving requires PagedAttention for efficient KV cache management, continuous batching to maximize GPU utilization, and optimized CUDA kernels.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 429,
        "title": "Containerize with Docker",
        "details": "## Overview\nLinux containers use namespaces for isolation (PID, network, mount, UTS, user) and cgroups for resource limits. The OCI spec defines container runtime standards.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 430,
        "title": "Set up CI/CD for model deployment",
        "details": "## Overview\nAutomated testing and deployment pipeline\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 431,
        "title": "Implement A/B testing infrastructure",
        "details": "## Overview\nCompare model versions in production\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 432,
        "title": "Implement prediction logging",
        "details": "## Overview\nLog inputs, outputs, latency for analysis\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 433,
        "title": "Set up data drift detection",
        "details": "## Overview\nMonitor input distribution changes over time\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 434,
        "title": "Implement model performance monitoring",
        "details": "## Overview\nTrack accuracy metrics when ground truth is available\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 435,
        "title": "Build alerting for model degradation",
        "details": "## Overview\nAutomated alerts when performance drops\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 436,
        "title": "Create retraining pipeline",
        "details": "## Overview\nAutomated or triggered retraining on new data\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "Transformers & LLMs Deep Dive",
    "pathId": 47,
    "tasks": [
      {
        "id": 437,
        "title": "Implement scaled dot-product attention from scratch",
        "details": "## Overview\nAttention mechanisms allow models to focus on relevant parts of the input when producing output. Self-attention computes relationships between all positions in a sequence, enabling context-aware representations.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 438,
        "title": "Implement multi-head attention",
        "details": "## Overview\nAttention mechanisms allow models to focus on relevant parts of the input when producing output. Self-attention computes relationships between all positions in a sequence, enabling context-aware representations.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 439,
        "title": "Build position encodings (sinusoidal and learned)",
        "details": "## Overview\nUnderstand why position information is needed\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 440,
        "title": "Implement a full transformer encoder block",
        "details": "## Overview\nTransformers are the foundation of modern NLP, using self-attention to process sequences in parallel. Understanding the architecture is essential for working with LLMs.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 441,
        "title": "Implement a transformer decoder with causal masking",
        "details": "## Overview\nTransformers are the foundation of modern NLP, using self-attention to process sequences in parallel. Understanding the architecture is essential for working with LLMs.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 442,
        "title": "Implement byte-pair encoding (BPE) from scratch",
        "details": "## Overview\nBuild vocabulary through iterative merging\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 443,
        "title": "Compare BPE, WordPiece, and SentencePiece",
        "details": "## Overview\nUnderstand tradeoffs between tokenization methods\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 444,
        "title": "Study how different models tokenize the same text",
        "details": "## Overview\nGPT-2, BERT, LLaMA tokenizer differences\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 445,
        "title": "Implement token embeddings with learned position encodings",
        "details": "## Overview\nEmbeddings map discrete tokens to dense vectors in continuous space. Position embeddings inject sequence order information since attention is position-invariant.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 446,
        "title": "Understand tokenization edge cases and their impact",
        "details": "## Overview\nNumbers, code, multilingual, special tokens\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 447,
        "title": "Implement causal language modeling (GPT-style)",
        "details": "## Overview\nPredict next token, cross-entropy loss\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 448,
        "title": "Implement masked language modeling (BERT-style)",
        "details": "## Overview\nRandom masking, bidirectional context\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 449,
        "title": "Study span corruption (T5-style)",
        "details": "## Overview\nUnderstand encoder-decoder pre-training\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 450,
        "title": "Implement a small language model and train on a corpus",
        "details": "## Overview\nFull training loop with proper evaluation\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 451,
        "title": "Analyze scaling laws: how performance changes with size",
        "details": "## Overview\nRead Kaplan et al. and Hoffmann et al. papers\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 452,
        "title": "Implement full fine-tuning on a classification task",
        "details": "## Overview\nFine-tuning adapts pretrained models to specific tasks. Full fine-tuning updates all parameters; parameter-efficient methods like LoRA update only a small subset.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 453,
        "title": "Implement LoRA (Low-Rank Adaptation)",
        "details": "## Overview\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable matrices to frozen pretrained weights, reducing memory and compute requirements by 10-100x.\n\n### Implementation\n```python\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank=8, alpha=16):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.scale = alpha / rank\n\n    def forward(self, x):\n        return (x @ self.A @ self.B) * self.scale\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 454,
        "title": "Implement QLoRA with quantization",
        "details": "## Overview\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable matrices to frozen pretrained weights, reducing memory and compute requirements by 10-100x.\n\n### Implementation\n```python\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank=8, alpha=16):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.scale = alpha / rank\n\n    def forward(self, x):\n        return (x @ self.A @ self.B) * self.scale\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 455,
        "title": "Study and implement instruction tuning",
        "details": "## Overview\nFormat data as instructions, train to follow them\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 456,
        "title": "Implement RLHF concepts: reward modeling and PPO basics",
        "details": "## Overview\nRLHF aligns models with human preferences through: 1) supervised fine-tuning on demonstrations, 2) reward model training on preferences, 3) RL optimization with KL penalty.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 457,
        "title": "Implement KV-cache for efficient autoregressive generation",
        "details": "## Overview\nKV caching stores computed Key and Value tensors from previous tokens during autoregressive generation, avoiding redundant computation and dramatically speeding up inference.\n\n### Implementation\n```python\ndef generate_with_cache(model, input_ids, max_new_tokens):\n    past_kv = None\n    for _ in range(max_new_tokens):\n        out = model(input_ids[:, -1:] if past_kv else input_ids, past_kv=past_kv)\n        past_kv = out.past_kv\n        next_token = out.logits[:, -1].argmax(dim=-1, keepdim=True)\n        input_ids = torch.cat([input_ids, next_token], dim=1)\n    return input_ids\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 458,
        "title": "Implement and compare sampling strategies",
        "details": "## Overview\nSampling strategies control generation diversity. Greedy picks highest probability, top-k samples from k best, top-p (nucleus) samples from smallest set with cumulative probability \u2265p.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 459,
        "title": "Implement quantization (INT8, INT4) with bitsandbytes",
        "details": "## Overview\nQuantization reduces model precision (FP32\u2192INT8\u2192INT4) to decrease memory usage and increase inference speed while maintaining acceptable accuracy.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 460,
        "title": "Study and implement speculative decoding",
        "details": "## Overview\nSpeculative decoding uses a fast draft model to generate candidate tokens, then verifies them in parallel with the large model, achieving 2-3x speedup for autoregressive generation.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 461,
        "title": "Profile inference and identify bottlenecks",
        "details": "## Overview\nMemory bandwidth, compute, attention costs\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 462,
        "title": "Implement RAG (Retrieval Augmented Generation)",
        "details": "## Overview\nRAG augments LLMs with external knowledge by embedding documents, retrieving relevant chunks for each query, and injecting them into the prompt context.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 463,
        "title": "Build a chat interface with conversation memory",
        "details": "## Overview\nManaging context window, summarization strategies\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 464,
        "title": "Implement function calling / tool use",
        "details": "## Overview\nParse structured output, execute functions, return results\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 465,
        "title": "Build an agent with planning and execution",
        "details": "## Overview\nLLM agents combine reasoning with tool use. They plan actions, execute tools, observe results, and iterate until the task is complete. Common patterns include ReAct and chain-of-thought.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 466,
        "title": "Implement guardrails and safety checks",
        "details": "## Overview\nInput validation, output filtering, content safety\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "Reimplement: Responder (LLMNR/NBT-NS)",
    "pathId": 65,
    "tasks": [
      {
        "id": 1096,
        "title": "Implement LLMNR responder",
        "details": "## Overview\nLink-Local Multicast\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1097,
        "title": "Build NBT-NS responder",
        "details": "## Overview\nNetBIOS Name Service\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1098,
        "title": "Add mDNS responder",
        "details": "## Overview\nDNS translates domain names to IP addresses. Implementation covers: packet parsing, recursive resolution, caching, and handling various record types (A, AAAA, CNAME, MX).\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1099,
        "title": "Implement WPAD responder",
        "details": "## Overview\nProxy auto-config\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1100,
        "title": "Build DHCPv6 server",
        "details": "## Overview\nIPv6 config\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1101,
        "title": "Add DNS poisoning",
        "details": "## Overview\nDNS translates domain names to IP addresses. Implementation covers: packet parsing, recursive resolution, caching, and handling various record types (A, AAAA, CNAME, MX).\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1102,
        "title": "Implement SMB server",
        "details": "## Overview\nCapture NTLM\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1103,
        "title": "Build HTTP server",
        "details": "## Overview\nHTTP servers parse requests, route to handlers, and format responses. Key considerations: keep-alive connections, chunked transfer encoding, and proper header handling.\n\n### Implementation\n```python\nimport socket\n\ndef handle_request(conn):\n    request = conn.recv(4096).decode()\n    method, path, _ = request.split('\\r\\n')[0].split(' ')\n    response = f\"HTTP/1.1 200 OK\\r\\nContent-Type: text/plain\\r\\n\\r\\nHello\"\n    conn.send(response.encode())\n    conn.close()\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1104,
        "title": "Add FTP server",
        "details": "## Overview\nFTP credentials\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1105,
        "title": "Implement LDAP server",
        "details": "## Overview\nDirectory auth\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1106,
        "title": "Build MSSQL server",
        "details": "## Overview\nDatabase auth\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1107,
        "title": "Add analysis mode",
        "details": "## Overview\nPassive capture\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "Reimplement: Complete Aircrack-ng Suite",
    "pathId": 88,
    "tasks": [
      {
        "id": 1797,
        "title": "Research the domain",
        "details": "## Overview\nUnderstand requirements\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1798,
        "title": "Design architecture",
        "details": "## Overview\nPlan components\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1799,
        "title": "Set up project structure",
        "details": "## Overview\nOrganize code\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1800,
        "title": "Implement core logic",
        "details": "## Overview\nMain functionality\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1801,
        "title": "Add error handling",
        "details": "## Overview\nRobust error management\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1802,
        "title": "Implement main features",
        "details": "## Overview\nCore use cases\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1803,
        "title": "Add configuration",
        "details": "## Overview\nRuntime settings\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1804,
        "title": "Build CLI or API",
        "details": "## Overview\nUser interface\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1805,
        "title": "Add logging",
        "details": "## Overview\nDebug and monitor\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1806,
        "title": "Write tests",
        "details": "## Overview\nVerify correctness\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1807,
        "title": "Optimize performance",
        "details": "## Overview\nProfile and improve\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1808,
        "title": "Add documentation",
        "details": "## Overview\nUsage and API docs\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1809,
        "title": "Handle edge cases",
        "details": "## Overview\nRobust handling\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1810,
        "title": "Package for distribution",
        "details": "## Overview\nEasy installation\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1811,
        "title": "Final testing",
        "details": "## Overview\nEnd-to-end verification\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "Build Your Own Container Runtime",
    "pathId": 95,
    "tasks": [
      {
        "id": 1249,
        "title": "Implement mount namespace",
        "details": "## Overview\nLinux containers use namespaces for isolation (PID, network, mount, UTS, user) and cgroups for resource limits. The OCI spec defines container runtime standards.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1250,
        "title": "Add UTS namespace",
        "details": "## Overview\nLinux containers use namespaces for isolation (PID, network, mount, UTS, user) and cgroups for resource limits. The OCI spec defines container runtime standards.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1257,
        "title": "Add image layer support",
        "details": "## Overview\nDocker-style layers\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  },
  {
    "pathName": "Transformers, LLMs & Generative AI Deep Dive",
    "pathId": 100,
    "tasks": [
      {
        "id": 930,
        "title": "Build multi-head attention",
        "details": "## Overview\nAttention mechanisms allow models to focus on relevant parts of the input when producing output. Self-attention computes relationships between all positions in a sequence, enabling context-aware representations.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 934,
        "title": "Understand GPT architecture",
        "details": "## Overview\nDecoder-only transformer\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 935,
        "title": "Implement token embeddings",
        "details": "## Overview\nEmbeddings map discrete tokens to dense vectors in continuous space. Position embeddings inject sequence order information since attention is position-invariant.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 936,
        "title": "Build language model head",
        "details": "## Overview\nPredict next token\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 937,
        "title": "Implement training loop",
        "details": "## Overview\nCausal LM training\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 939,
        "title": "Build KV cache",
        "details": "## Overview\nKV caching stores computed Key and Value tensors from previous tokens during autoregressive generation, avoiding redundant computation and dramatically speeding up inference.\n\n### Implementation\n```python\ndef generate_with_cache(model, input_ids, max_new_tokens):\n    past_kv = None\n    for _ in range(max_new_tokens):\n        out = model(input_ids[:, -1:] if past_kv else input_ids, past_kv=past_kv)\n        past_kv = out.past_kv\n        next_token = out.logits[:, -1].argmax(dim=-1, keepdim=True)\n        input_ids = torch.cat([input_ids, next_token], dim=1)\n    return input_ids\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 941,
        "title": "Implement LoRA",
        "details": "## Overview\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable matrices to frozen pretrained weights, reducing memory and compute requirements by 10-100x.\n\n### Implementation\n```python\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank=8, alpha=16):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.scale = alpha / rank\n\n    def forward(self, x):\n        return (x @ self.A @ self.B) * self.scale\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 943,
        "title": "Add RLHF basics",
        "details": "## Overview\nRLHF aligns models with human preferences through: 1) supervised fine-tuning on demonstrations, 2) reward model training on preferences, 3) RL optimization with KL penalty.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 944,
        "title": "Implement quantization",
        "details": "## Overview\nQuantization reduces model precision (FP32\u2192INT8\u2192INT4) to decrease memory usage and increase inference speed while maintaining acceptable accuracy.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 945,
        "title": "Build serving infrastructure",
        "details": "## Overview\nHigh-throughput LLM serving requires PagedAttention for efficient KV cache management, continuous batching to maximize GPU utilization, and optimized CUDA kernels.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1231,
        "title": "Implement multi-head attention",
        "details": "## Overview\nAttention mechanisms allow models to focus on relevant parts of the input when producing output. Self-attention computes relationships between all positions in a sequence, enabling context-aware representations.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1232,
        "title": "Build positional encoding",
        "details": "## Overview\nSinusoidal and learned\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1233,
        "title": "Add layer normalization",
        "details": "## Overview\nPre-LN vs Post-LN\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1234,
        "title": "Implement feed-forward network",
        "details": "## Overview\nMLP layers\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1235,
        "title": "Build residual connections",
        "details": "## Overview\nSkip connections\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1236,
        "title": "Implement gradient accumulation",
        "details": "## Overview\nGradients indicate the direction and magnitude of steepest increase for a function. In optimization, we move opposite to gradients (descent) to minimize loss.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1237,
        "title": "Add learning rate warmup",
        "details": "## Overview\nGradual LR increase\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1238,
        "title": "Build mixed precision training",
        "details": "## Overview\nFP16/BF16\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1239,
        "title": "Implement gradient clipping",
        "details": "## Overview\nGradients indicate the direction and magnitude of steepest increase for a function. In optimization, we move opposite to gradients (descent) to minimize loss.\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1240,
        "title": "Add checkpoint saving",
        "details": "## Overview\nResume training\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1241,
        "title": "Implement LoRA",
        "details": "## Overview\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable matrices to frozen pretrained weights, reducing memory and compute requirements by 10-100x.\n\n### Implementation\n```python\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank=8, alpha=16):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.scale = alpha / rank\n\n    def forward(self, x):\n        return (x @ self.A @ self.B) * self.scale\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1242,
        "title": "Build QLoRA",
        "details": "## Overview\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable matrices to frozen pretrained weights, reducing memory and compute requirements by 10-100x.\n\n### Implementation\n```python\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank=8, alpha=16):\n        super().__init__()\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.scale = alpha / rank\n\n    def forward(self, x):\n        return (x @ self.A @ self.B) * self.scale\n```\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1243,
        "title": "Add prompt tuning",
        "details": "## Overview\nSoft prompts\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1244,
        "title": "Implement adapter layers",
        "details": "## Overview\nBottleneck adapters\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      },
      {
        "id": 1245,
        "title": "Build instruction tuning pipeline",
        "details": "## Overview\nFollowing instructions\n\n### Implementation\nFollow the implementation steps in the task description. Start with a minimal working version, then iterate.\n\n### Key Concepts\n- Understand the core algorithm before coding\n- Handle edge cases explicitly\n- Test against reference implementations\n- Profile for performance bottlenecks\n\n### Practice\n- [ ] Implement from scratch without references\n- [ ] Test with edge cases\n- [ ] Compare output with established libraries\n- [ ] Optimize for production use\n\n### Completion Criteria\n- [ ] Code produces correct output\n- [ ] Can explain the implementation to others\n- [ ] Edge cases handled properly\n- [ ] Performance is acceptable"
      }
    ]
  }
]